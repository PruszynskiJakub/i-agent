Title: Open Source Prompt Management - Langfuse

DocsPrompt ManagementGet Started

Prompt Management
=================

Use Langfuse to effectively **manage** and **version** your prompts. Langfuse prompt management is a **Prompt CMS** (Content Management System).

Version ControlDeployMetricsTest in PlaygroundLink with Traces

Collaboratively version and edit prompts via UI, API, or SDKs.

Deploy prompts to production or any environment via labels - without any code changes.

Compare latency, cost, and evaluation metrics across different versions of your prompts.

Instantly test your prompts in the playground.

Link prompts with traces to understand how they perform in the context of your LLM application.

What is prompt management?[](https://langfuse.com/docs/prompts/get-started#what-is-prompt-management)
-----------------------------------------------------------------------------------------------------

**Prompt management is a systematic approach to storing, versioning and retrieving prompts in LLM applications.** Key aspects of prompt management include version control, decoupling prompts from code, monitoring, logging and optimizing prompts as well as integrating prompts with the rest of your application and tool stack.

Why use prompt management?[](https://langfuse.com/docs/prompts/get-started#why-use-prompt-management)
-----------------------------------------------------------------------------------------------------

> Canâ€™t I just hardcode my prompts in my application and track them in Git? Yes, wellâ€¦ you can and all of us have done it.

Typical benefits of using a CMS apply here:

*   Decoupling: deploy new prompts without redeploying your application.
*   Non-technical users can create and update prompts via Langfuse Console.
*   Quickly rollback to a previous version of a prompt.

Platform benefits:

*   Track performance of prompt versions in [Langfuse Tracing](https://langfuse.com/docs/tracing).

Performance benefits compared to other implementations:

*   No latency impact after first use of a prompt due to client-side caching and asynchronous cache refreshing.
*   Support for text and chat prompts.
*   Edit/manage via UI, SDKs, or API.

Prompt Engineering FAQ[](https://langfuse.com/docs/prompts/get-started#prompt-engineering-faq)
----------------------------------------------------------------------------------------------

### What is prompt engineering?

### How to measure prompt performance?

Langfuse prompt object[](https://langfuse.com/docs/prompts/get-started#langfuse-prompt-object)
----------------------------------------------------------------------------------------------

Example prompt in Langfuse with custom config



{
â€œnameâ€: â€œmovie-criticâ€,
â€œtypeâ€: â€œtextâ€,
â€œpromptâ€: â€œAs a {{criticLevel}} movie critic, do you like {{movie}}?â€,
â€œconfigâ€: {
â€œmodelâ€: â€œgpt-3.5-turboâ€,
â€œtemperatureâ€: 0.5,
â€œsupported_languagesâ€: [â€œenâ€, â€œfrâ€]
},
â€œversionâ€: 1,
â€œlabelsâ€: [â€œproductionâ€, â€œstagingâ€, â€œlatestâ€],
â€œtagsâ€: [â€œmoviesâ€]
}


*   `name`: Unique name of the prompt within a Langfuse project.
*   `type`: The type of the prompt content (`text` or `chat`). Default is `text`.
*   `prompt`: The text template with variables (e.g. `This is a prompt with a {{variable}}`). For chat prompts, this is a list of chat messages each with `role` and `content`.
*   `config`: Optional JSON object to store any parameters (e.g. model parameters or model tools).
*   `version`: Integer to indicate the version of the prompt. The version is automatically incremented when creating a new prompt version.
*   `labels`: Labels that can be used to fetch specific prompt versions in the SDKs.
    *   When using a prompt without specifying a label, Langfuse will serve the version with the `production` label.
    *   `latest` points to the most recently created version.
    *   You can create any additional labels, e.g. for different environments (`staging`, `production`) or tenants (`tenant-1`, `tenant-2`).

How it works[](https://langfuse.com/docs/prompts/get-started#how-it-works)
--------------------------------------------------------------------------

### Create/update prompt[](https://langfuse.com/docs/prompts/get-started#createupdate-prompt)

If you already have a prompt with the same `name`, the prompt will be added as a new version.

Langfuse UIPythonJS/TS


Create a text prompt

langfuse.create_prompt(
name=â€œmovie-criticâ€,
type=â€œtextâ€,
prompt=â€œAs a {{criticlevel}} movie critic, do you like {{movie}}?â€,
labels=[â€œproductionâ€], # directly promote to production
config={
â€œmodelâ€: â€œgpt-3.5-turboâ€,
â€œtemperatureâ€: 0.7,
â€œsupported_languagesâ€: [â€œenâ€, â€œfrâ€],
}, # optionally, add configs (e.g. model parameters or model tools) or tags
)

Create a chat prompt

langfuse.create_prompt(
name=â€œmovie-critic-chatâ€,
type=â€œchatâ€,
prompt=[
{ â€œroleâ€: â€œsystemâ€, â€œcontentâ€: â€œYou are an {{criticlevel}} movie criticâ€ },
{ â€œroleâ€: â€œuserâ€, â€œcontentâ€: â€œDo you like {{movie}}?â€ },
],
labels=[â€œproductionâ€], # directly promote to production
config={
â€œmodelâ€: â€œgpt-3.5-turboâ€,
â€œtemperatureâ€: 0.7,
â€œsupported_languagesâ€: [â€œenâ€, â€œfrâ€],
}, # optionally, add configs (e.g. model parameters or model tools) or tags
)


If you already have a prompt with the same name, the prompt will be added as a new version.



// Create a text prompt
await langfuse.createPrompt({
name: â€œmovie-criticâ€,
type: â€œtextâ€,
prompt: â€œAs a {{criticlevel}} critic, do you like {{movie}}?â€,
labels: [â€œproductionâ€], // directly promote to production
config: {
model: â€œgpt-3.5-turboâ€,
temperature: 0.7,
supported_languages: [â€œenâ€, â€œfrâ€],
}, // optionally, add configs (e.g. model parameters or model tools) or tags
});

// Create a chat prompt
await langfuse.createPrompt({
name: â€œmovie-critic-chatâ€,
type: â€œchatâ€,
prompt: [
{ role: â€œsystemâ€, content: â€œYou are an {{criticlevel}} movie criticâ€ },
{ role: â€œuserâ€, content: â€œDo you like {{movie}}?â€ },
],
labels: [â€œproductionâ€], // directly promote to production
config: {
model: â€œgpt-3.5-turboâ€,
temperature: 0.7,
supported_languages: [â€œenâ€, â€œfrâ€],
}, // optionally, add configs (e.g. model parameters or model tools) or tags
});


If you already have a prompt with the same name, the prompt will be added as a new version.

### Use prompt[](https://langfuse.com/docs/prompts/get-started#use-prompt)

At runtime, you can fetch the latest production version from Langfuse.

PythonJS/TSLangchain (Python)Langchain (JS)



from langfuse import Langfuse

Initialize Langfuse client

langfuse = Langfuse()

Get current production version of a text prompt

prompt = langfuse.get_prompt(â€œmovie-criticâ€)

Insert variables into prompt template

compiled_prompt = prompt.compile(criticlevel=â€œexpertâ€, movie=â€œDune 2â€)

-> â€œAs an expert movie critic, do you like Dune 2?â€

Chat prompts


Get current production version of a chat prompt

chat_prompt = langfuse.get_prompt(â€œmovie-critic-chatâ€, type=â€œchatâ€) # type arg infers the prompt type (default is â€˜textâ€™)

Insert variables into chat prompt template

compiled_chat_prompt = chat_prompt.compile(criticlevel=â€œexpertâ€, movie=â€œDune 2â€)

-> [{â€œroleâ€: â€œsystemâ€, â€œcontentâ€: â€œYou are an expert movie criticâ€}, {â€œroleâ€: â€œuserâ€, â€œcontentâ€: â€œDo you like Dune 2?â€}]

Optional parameters


Get specific version

prompt = langfuse.get_prompt(â€œmovie-criticâ€, version=1)

Get specific label

prompt = langfuse.get_prompt(â€œmovie-criticâ€, label=â€œstagingâ€)

Get latest prompt version. The â€˜latestâ€™ label is automatically maintained by Langfuse.

prompt = langfuse.get_prompt(â€œmovie-criticâ€, label=â€œlatestâ€)

Extend cache TTL from default 60 to 300 seconds

prompt = langfuse.get_prompt(â€œmovie-criticâ€, cache_ttl_seconds=300)

Number of retries on fetching prompts from the server. Default is 2.

prompt = langfuse.get_prompt(â€œmovie-criticâ€, max_retries=3)

Timeout per call to the Langfuse API in seconds. Default is 20.

prompt = langfuse.get_prompt(â€œmovie-criticâ€, fetch_timeout_seconds=3)


Attributes


Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.

prompt.prompt

Config object

prompt.config




import { Langfuse } from â€œlangfuseâ€;

// Iniitialize the Langfuse client
const langfuse = new Langfuse();

// Get current production version
const prompt = await langfuse.getPrompt(â€œmovie-criticâ€);

// Insert variables into prompt template
const compiledPrompt = prompt.compile({
criticlevel: â€œexpertâ€,
movie: â€œDune 2â€,
});
// -> â€œAs an expert movie critic, do you like Dune 2?â€


Chat prompts



// Get current production version of a chat prompt
const chatPrompt = await langfuse.getPrompt(â€œmovie-critic-chatâ€, undefined, {
type: â€œchatâ€,
}); // type option infers the prompt type (default is â€˜textâ€™)

// Insert variables into chat prompt template
const compiledChatPrompt = chatPrompt.compile({
criticlevel: â€œexpertâ€,
movie: â€œDune 2â€,
});
// -> [{â€œroleâ€: â€œsystemâ€, â€œcontentâ€: â€œYou are an expert movie criticâ€}, {â€œroleâ€: â€œuserâ€, â€œcontentâ€: â€œDo you like Dune 2?â€}]


Optional parameters



// Get specific version of a prompt (here version 1)
const prompt = await langfuse.getPrompt(â€œmovie-criticâ€, 1);

// Get specific label
const prompt = await langfuse.getPrompt(â€œmovie-criticâ€, undefined, {
label: â€œstagingâ€,
});

// Get latest prompt version. The â€˜latestâ€™ label is automatically maintained by Langfuse.
const prompt = await langfuse.getPrompt(â€œmovie-criticâ€, undefined, {
label: â€œlatestâ€,
});

// Extend cache TTL from default 1 to 5 minutes
const prompt = await langfuse.getPrompt(â€œmovie-criticâ€, undefined, {
cacheTtlSeconds: 300,
});

// Number of retries on fetching prompts from the server. Default is 2.
const promptWithMaxRetries = await langfuse.getPrompt(
â€œmovie-criticâ€,
undefined,
{
maxRetries: 5,
}
);

// Timeout per call to the Langfuse API in milliseconds. Default is 10 seconds.
const promptWithFetchTimeout = await langfuse.getPrompt(
â€œmovie-criticâ€,
undefined,
{
fetchTimeoutMs: 5000,
}
);


Attributes



// Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.
prompt.prompt;

// Config object
prompt.config;


As Langfuse and Langchain process input variables of prompt templates differently (`{}` instead of `{{}}`), we provide the `prompt.get_langchain_prompt()` method to transform the Langfuse prompt into a string that can be used with Langchainâ€™s PromptTemplate. You can pass optional keyword arguments to `prompt.get_langchain_prompt(**kwargs)` in order to precompile some variables and handle the others with Langchainâ€™s PromptTemplate.



from langfuse import Langfuse
from langchain_core.prompts import ChatPromptTemplate

Initialize Langfuse client

langfuse = Langfuse()

Get current production version

langfuse_prompt = langfuse.get_prompt(â€œmovie-criticâ€)

Example using ChatPromptTemplate

langchain_prompt = ChatPromptTemplate.from_template(langfuse_prompt.get_langchain_prompt())

Example using ChatPromptTemplate with pre-compiled variables.

langchain_prompt = ChatPromptTemplate.from_template(langfuse_prompt.get_langchain_prompt(strictness=â€˜toughâ€™))


Chat prompts


Get current production version of a chat prompt

langfuse_prompt = langfuse.get_prompt(â€œmovie-critic-chatâ€, type=â€œchatâ€)

Create a Langchain ChatPromptTemplate from the Langfuse prompt chat messages

langchain_prompt = ChatPromptTemplate.from_messages(langfuse_prompt.get_langchain_prompt())


Optional parameters


Get specific version

prompt = langfuse.get_prompt(â€œmovie-criticâ€, version=1)

Get specific label

prompt = langfuse.get_prompt(â€œmovie-criticâ€, label=â€œstagingâ€)

Get latest prompt version. The â€˜latestâ€™ label is automatically maintained by Langfuse.

prompt = langfuse.get_prompt(â€œmovie-criticâ€, label=â€œlatestâ€)

Extend cache TTL from default 60 to 300 seconds

prompt = langfuse.get_prompt(â€œmovie-criticâ€, cache_ttl_seconds=300)


Attributes


Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.

prompt.prompt

Config object

prompt.config


As Langfuse and Langchain process input variables of prompt templates differently (`{}` instead of `{{}}`), we provide the `prompt.getLangchainPrompt()` method to transform the Langfuse prompt into a string that can be used with Langchainâ€™s PromptTemplate.



import { Langfuse } from â€œlangfuseâ€;
import { ChatPromptTemplate } from â€œ@langchain/core/promptsâ€;

const langfuse = new Langfuse();

// Get current production version
const langfusePrompt = await langfuse.getPrompt(â€œmovie-criticâ€);

// Example using ChatPromptTemplate
const promptTemplate = PromptTemplate.fromTemplate(
langfusePrompt.getLangchainPrompt()
);


Chat prompts



// Get current production version of a chat prompt
const langfusePrompt = await langfuse.getPrompt(
â€œmovie-critic-chatâ€,
undefined,
{ type: â€œchatâ€ }
);

// Example using ChatPromptTemplate
const promptTemplate = ChatPromptTemplate.fromMessages(
langfusePrompt.getLangchainPrompt().map((msg) => [msg.role, msg.content])
);


Optional parameters



// Get specific version of a prompt (here version 1)
const prompt = await langfuse.getPrompt(â€œmovie-criticâ€, 1);

// Get specific label
const prompt = await langfuse.getPrompt(â€œmovie-criticâ€, undefined, {
label: â€œstagingâ€,
});

// Get latest prompt version. The â€˜latestâ€™ label is automatically maintained by Langfuse.
const prompt = await langfuse.getPrompt(â€œmovie-criticâ€, undefined, {
label: â€œlatestâ€,
});

// Extend cache TTL from default 1 to 5 minutes
const prompt = await langfuse.getPrompt(â€œmovie-criticâ€, undefined, {
cacheTtlSeconds: 300,
});


Attributes



// Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.
prompt.prompt;

// Config object
prompt.config;


### Link with Langfuse Tracing (optional)[](https://langfuse.com/docs/prompts/get-started#link-with-langfuse-tracing-optional)

Add the prompt object to the `generation` call in the SDKs to link the generation in [Langfuse Tracing](https://langfuse.com/docs/tracing) to the prompt version. This linkage enables tracking of metrics by prompt version and name, such as â€œmovie-criticâ€, directly in the Langfuse UI. Metrics like scores per prompt version provide insights into how modifications to prompts impact the quality of the generations. If a [fallback prompt](https://langfuse.com/docs/prompts/get-started#fallback) is used, no link will be created.

This is currently unavailable when using the LlamaIndex integration.

Python SDKJS/TS SDKOpenAI SDK (Python)OpenAI SDK (JS/TS)Langchain (Python)Langchain (JS/TS)Vercel AI SDK

Decorators



from langfuse.decorators import langfuse_context, observe

@observe(as_type=â€œgenerationâ€)
def nested_generation():
prompt = langfuse.get_prompt(â€œmovie-criticâ€)

langfuse_context.update_current_observation(
    prompt=prompt,
)


@observe()
def main():
nested_generation()

main()


Low-level SDK



langfuse.generation(
â€¦

prompt=prompt
â€¦
)



langfuse.generation({
â€¦

prompt: prompt
â€¦
})



prompt = langfuse.get_prompt(â€œcalculatorâ€)

openai.chat.completions.create(
model=â€œgpt-3.5-turboâ€,
messages=[
{â€œroleâ€: â€œsystemâ€, â€œcontentâ€: prompt.compile(base=10)},
{â€œroleâ€: â€œuserâ€, â€œcontentâ€: "1 + 1 = "}],
langfuse_prompt=prompt
)




import { observeOpenAI } from â€œlangfuseâ€;
import OpenAI from â€œopenaiâ€;

const langfusePrompt = await langfuse.getPrompt(â€œprompt-nameâ€); // Fetch a previously created prompt

const res = await observeOpenAI(new OpenAI(), {
langfusePrompt,
}).completions.create({
prompt: langfusePrompt.prompt,
model: â€œgpt-3.5-turbo-instructâ€,
max_tokens: 300,
});




from langfuse import Langfuse
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_openai import ChatOpenAI, OpenAI

langfuse = Langfuse()

Text prompts:

langfuse_text_prompt = langfuse.get_prompt(â€œmovie-criticâ€)

Pass the langfuse_text_prompt to the PromptTemplate as metadata to link it to generations that use it

langchain_text_prompt = PromptTemplate.from_template(
langfuse_text_prompt.get_langchain_prompt(),
metadata={â€œlangfuse_promptâ€: langfuse_text_prompt},
)

Use the text prompt in a Langchain chain

llm = OpenAI()
completion_chain = langchain_text_prompt | llm

completion_chain.invoke({â€œmovieâ€: â€œDune 2â€, â€œcriticlevelâ€: â€œexpertâ€})

Chat prompts:

langfuse_chat_prompt = langfuse.get_prompt(â€œmovie-critic-chatâ€, type=â€œchatâ€)

Manually set the metadata on the langchain_chat_prompt to link it to generations that use it

langchain_chat_prompt = ChatPromptTemplate.from_messages(
langfuse_chat_prompt.get_langchain_prompt()
)
langchain_chat_prompt.metadata = {â€œlangfuse_promptâ€: langfuse_chat_prompt}

Use the chat prompt in a Langchain chain

chat_llm = ChatOpenAI()
chat_chain = langchain_chat_prompt | chat_llm

chat_chain.invoke({â€œmovieâ€: â€œDune 2â€, â€œcriticlevelâ€: â€œexpertâ€})


If you use the `with_config` method on the PromptTemplate to create a new Langchain Runnable with updated config, please make sure to pass the `langfuse_prompt` in the `metadata` key as well.

Set the `langfuse_prompt` metadata key only on PromptTemplates and not additionally on the LLM calls or elsewhere in your chains.



import { Langfuse } from â€œlangfuseâ€;
import { PromptTemplate } from â€œ@langchain/core/promptsâ€;
import { ChatOpenAI, OpenAI } from â€œ@langchain/openaiâ€;

const langfuse = new Langfuse();

// Text prompts
const langfuseTextPrompt = await langfuse.getPrompt(â€œmovie-criticâ€); // Fetch a previously created text prompt

// Pass the langfuseTextPrompt to the PromptTemplate as metadata to link it to generations that use it
const langchainTextPrompt = PromptTemplate.fromTemplate(
langfuseTextPrompt.getLangchainPrompt()
).withConfig({
metadata: { langfusePrompt: langfuseTextPrompt },
});

const model = new OpenAI();
const chain = langchainTextPrompt.pipe(model);

await chain.invoke({ movie: â€œDune 2â€, criticlevel: â€œexpertâ€ });

// Chat prompts
const langfuseChatPrompt = await langfuse.getPrompt(
â€œmovie-critic-chatâ€,
undefined,
{
type: â€œchatâ€,
}
); // type option infers the prompt type as chat (default is â€˜textâ€™)

const langchainChatPrompt = ChatPromptTemplate.fromMessages(
langfuseChatPrompt.getLangchainPrompt().map((m) => [m.role, m.content])
).withConfig({
metadata: { langfusePrompt: langfuseTextPrompt },
});

const chatModel = new ChatOpenAI();
const chatChain = langchainChatPrompt.pipe(chatModel);

await chatChain.invoke({ movie: â€œDune 2â€, criticlevel: â€œexpertâ€ });


Link Langfuse prompts to Vercel AI SDK generations by setting the `langfusePrompt` property in the `metadata` field:



import { generateText } from â€œaiâ€;
import { Langfuse } from â€œlangfuseâ€;

const langfuse = new Langfuse();

const fetchedPrompt = await langfuse.getPrompt(â€œmy-promptâ€);

const result = await generateText({
model: openai(â€œgpt-4oâ€),
prompt: fetchedPrompt.prompt,
experimental_telemetry: {
isEnabled: true,
metadata: {
langfusePrompt: fetchedPrompt.toJSON(),
},
},
});


### Rollbacks (optional)[](https://langfuse.com/docs/prompts/get-started#rollbacks-optional)

When a prompt has a `production` label, then that version will be served by default in the SDKs. You can quickly rollback to a previous version by setting the `production` label to that previous version in the Langfuse UI.

End-to-end examples[](https://langfuse.com/docs/prompts/get-started#end-to-end-examples)
----------------------------------------------------------------------------------------

The following example notebooks include end-to-end examples of prompt management:

[Example OpenAI Functions](https://langfuse.com/docs/prompts/example-openai-functions)[Example Langchain (Python)](https://langfuse.com/docs/prompts/example-langchain)[Example Langchain (JS/TS)](https://langfuse.com/docs/prompts/example-langchain-js)

We also used Prompt Management for our Docs Q&A Chatbot and traced it with Langfuse. You can get view-only access to the project by signing up to the [public demo](https://langfuse.com/docs/demo).

Caching in client SDKs[](https://langfuse.com/docs/prompts/get-started#caching-in-client-sdks)
----------------------------------------------------------------------------------------------

Langfuse prompts are served from a client-side cache in the SDKs. Therefore, **Langfuse Prompt Management does not add any latency to your application when a cached prompt is available from a previous use**. Optionally, you can pre-fetch prompts on application startup to ensure that the cache is populated (example below).

### Optional: Pre-fetch prompts on application start[](https://langfuse.com/docs/prompts/get-started#optional-pre-fetch-prompts-on-application-start)

**To ensure that your application never hits an empty cache at runtime** (and thus adding an initial delay of fetching the prompt), you can pre-fetch the prompts during the application startup. This pre-fetching will populate the cache and ensure that the prompts are readily available when needed.

_Example implementations:_

Python (Flask)JS/TS (Express)



from flask import Flask, jsonify
from langfuse import Langfuse

Initialize the Flask app and Langfuse client

app = Flask(name)
langfuse = Langfuse()

def fetch_prompts_on_startup():
# Fetch and cache the production version of the prompt
langfuse.get_prompt(â€œmovie-criticâ€)

Call the function during application startup

fetch_prompts_on_startup()

@app.route(â€˜/get-movie-prompt/â€™, methods=[â€˜GETâ€™])
def get_movie_prompt(movie):
prompt = langfuse.get_prompt(â€œmovie-criticâ€)
compiled_prompt = prompt.compile(criticlevel=â€œexpertâ€, movie=movie)
return jsonify({â€œpromptâ€: compiled_prompt})

if name == â€˜mainâ€™:
app.run(debug=True)




import express from â€œexpressâ€;
import { Langfuse } from â€œlangfuseâ€;

// Initialize the Express app and Langfuse client
const app = express();
const langfuse = new Langfuse();

async function fetchPromptsOnStartup() {
// Fetch and cache the production version of the prompt
await langfuse.getPrompt(â€œmovie-criticâ€);
}

// Call the function during application startup
fetchPromptsOnStartup();

app.get(â€œ/get-movie-prompt/:movieâ€, async (req, res) => {
const movie = req.params.movie;
const prompt = await langfuse.getPrompt(â€œmovie-criticâ€);
const compiledPrompt = prompt.compile({ criticlevel: â€œexpertâ€, movie });
res.json({ prompt: compiledPrompt });
});

app.listen(3000, () => {
console.log(â€œServer is running on port 3000â€);
});


### Optional: Customize caching duration (TTL)[](https://langfuse.com/docs/prompts/get-started#optional-customize-caching-duration-ttl)

The caching duration is configurable if you wish to reduce network overhead of the Langfuse Client. The default cache TTL is 60 seconds. After the TTL expires, the SDKs will refetch the prompt in the background and update the cache. Refetching is done asynchronously and does not block the application.

PythonJS/TS


Get current production prompt version and cache for 5 minutes

prompt = langfuse.get_prompt(â€œmovie-criticâ€, cache_ttl_seconds=300)




// Get current production version and cache prompt for 5 minutes
const prompt = await langfuse.getPrompt(â€œmovie-criticâ€, undefined, {
cacheTtlSeconds: 300,
});


### Optional: Disable caching[](https://langfuse.com/docs/prompts/get-started#disable-caching)

You can disable caching by setting the `cacheTtlSeconds` to `0`. This will ensure that the prompt is fetched from the Langfuse API on every call. This is recommended for non-production use cases where you want to ensure that the prompt is always up to date with the latest version in Langfuse.

PythonJS/TS



prompt = langfuse.get_prompt(â€œmovie-criticâ€, cache_ttl_seconds=0)

Common in non-production environments, no cache + latest version

prompt = langfuse.get_prompt(â€œmovie-criticâ€, cache_ttl_seconds=0, label=â€œlatestâ€)




const prompt = await langfuse.getPrompt(â€œmovie-criticâ€, undefined, {
cacheTtlSeconds: 0,
});

// Common in non-production environments, no cache + latest version
const prompt = await langfuse.getPrompt(â€œmovie-criticâ€, undefined, {
cacheTtlSeconds: 0,
label: â€œlatestâ€,
});


### Performance measurement of inital fetch (empty client-side cache)[](https://langfuse.com/docs/prompts/get-started#performance-measurement-of-inital-fetch-empty-client-side-cache)

We measured the execution time of the following snippet with fully disabled caching.



prompt = langfuse.get_prompt(â€œperf-testâ€)
prompt.compile(input=â€œtestâ€)


Results from 1000 sequential executions in a local jupyter notebook using Langfuse Cloud (includes network latency):

![Image 7: Performance Chart](https://langfuse.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fprompt-performance-chart.3c3545c4.png&w=1200&q=75)



count 1000.000000
mean 0.178465 sec
std 0.058125 sec
min 0.137314 sec
25% 0.161333 sec
50% 0.165919 sec
75% 0.171736 sec
max 0.687994 sec


Optional: Guaranteed availability[](https://langfuse.com/docs/prompts/get-started#optional-guaranteed-availability)
-------------------------------------------------------------------------------------------------------------------

ğŸ’¡

Implementing this is usually not necessary as it adds complexity to your application and the Langfuse API is highly available. However, if you require 100% availability, you can use the following options.

The Langfuse API has high uptime and prompts are cached locally in the SDKs to prevent network issues from affecting your application.

However, `get_prompt()`/`getPrompt()` will throw an exception if:

1.  No local (fresh or stale) cached prompt is available -\> new application instance fetching prompt for the first time
2.  _and_ network request fails -\> networking or Langfuse API issue (after retries)

To gurantee 100% availability, there are two options:

1.  Pre-fetch prompts on application startup and exit the application if the prompt is not available.
2.  Provide a `fallback` prompt that will be used in these cases.

### Option 1: Pre-fetch prompts on application startup and exit if not available[](https://langfuse.com/docs/prompts/get-started#option-1-pre-fetch-prompts-on-application-startup-and-exit-if-not-available)

PythonJS/TS



from langfuse import Langfuse
import sys

Initialize Langfuse client

langfuse = Langfuse()

def fetch_prompts_on_startup():
try:
# Fetch and cache the production version of the prompt
langfuse.get_prompt(â€œmovie-criticâ€)
except Exception as e:
print(f"Failed to fetch prompt on startup: {e}")
sys.exit(1) # Exit the application if the prompt is not available

Call the function during application startup

fetch_prompts_on_startup()

Your application code here



import { Langfuse } from â€œlangfuseâ€;

// Initialize Langfuse client
const langfuse = new Langfuse();

async function fetchPromptsOnStartup() {
try {
// Fetch and cache the production version of the prompt
await langfuse.getPrompt(â€œmovie-criticâ€);
} catch (error) {
console.error(â€œFailed to fetch prompt on startup:â€, error);
process.exit(1); // Exit the application if the prompt is not available
}
}

// Call the function during application startup
fetchPromptsOnStartup();

// Your application code here


### Option 2: Fallback[](https://langfuse.com/docs/prompts/get-started#fallback)

PythonJS/TS



from langfuse import Langfuse
langfuse = Langfuse()

Get text prompt with fallback

prompt = langfuse.get_prompt(
â€œmovie-criticâ€,
fallback=â€œDo you like {{movie}}?â€
)

Get chat prompt with fallback

chat_prompt = langfuse.get_prompt(
â€œmovie-critic-chatâ€,
type=â€œchatâ€,
fallback=[{â€œroleâ€: â€œsystemâ€, â€œcontentâ€: â€œYou are an expert on {{movie}}â€}]
)

True if the prompt is a fallback

prompt.is_fallback




import { Langfuse } from â€œlangfuseâ€;
const langfuse = new Langfuse();

// Get text prompt with fallback
const prompt = await langfuse.getPrompt(â€œmovie-criticâ€, undefined, {
fallback: â€œDo you like {{movie}}?â€,
});

// Get chat prompt with fallback
const chatPrompt = await langfuse.getPrompt(â€œmovie-critic-chatâ€, undefined, {
type: â€œchatâ€,
fallback: [{ role: â€œsystemâ€, content: â€œYou are an expert on {{movie}}â€ }],
});

// True if the prompt is a fallback
prompt.isFallback;

